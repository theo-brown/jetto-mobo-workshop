{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `jetto-mobo`: simple Bayesian optimisation for designing plasma profiles\n",
    "\n",
    "Welcome to this tutorial on using Bayesian optimisation for plasma profile design! \n",
    "Our package, `jetto-mobo`, is designed to make using state-of-the-art model-based optimisation routines as painless as possible.\n",
    "\n",
    "Documentation: https://jetto-mobo.readthedocs.io/en/latest/\n",
    "\n",
    "Pre-print: https://arxiv.org/abs/2310.02669\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"assets/diagram_1.png\" width=\"50%\">\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Overview of the method\n",
    "\n",
    "2. Running JETTO (or not)\n",
    "\n",
    "3. Parameterising input profiles (+ exercise)\n",
    "\n",
    "4. Formulating objective functions (+ exercise)\n",
    "\n",
    "Exercise: Interim tests\n",
    "\n",
    "5. Generating initial samples\n",
    "\n",
    "6. Fitting a model (+ exercise)\n",
    "\n",
    "7. Optimising an acquisition function (+ exercise)\n",
    "\n",
    "8. Extracting the Pareto optimal solutions\n",
    "\n",
    "Exercise: Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a minute or so to run\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of the method\n",
    "\n",
    "Bayesian optimisation is a *model-based* optimisation scheme, that uses a probabilistic model to make decisions about what points to trial next. It has been shown to outperform other gradient-free optimisation schemes in a wide variety of tasks. Check out the [BayesOpt section of the documentation](https://jetto-mobo.readthedocs.io/en/latest/bayesopt.html) for more detail!\n",
    "\n",
    "In short, we use a Gaussian process (GP) to learn the mapping from input parameters (e.g. the shape of an ECRH profile) to objective functions (e.g. the shape of the $q$-profile). We can then use the GP's predictive distribution to optimise an acquisition function, which tells us which points we should try next.\n",
    "\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"assets/bo_flowchart.svg\" width=\"75%\">\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "Our package is specifically designed for *multi-objective* optimisation, where there are multiple competing objectives that need to be optimised. Tackling multi-objective optimisation tasks in the right way can produce really illuminating results that can help shape the decision-making process, as we can gain an understanding about how the objectives interact.\n",
    "Again, there's a bit more detail in the [multi-objective optimisation section of the documentation](https://jetto-mobo.readthedocs.io/en/latest/multiobjective.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running JETTO (or not)\n",
    "\n",
    "Because JETTO takes ~3 hours to converge to a new steady-state configuration, we've provided a quick-and-dirty replacement that makes up a semi-plausible $q$ profile from an input ECRH profile. In this notebook we'll use this replacement, `workshop_tools.run_toy`, instead of the 'real' function, `jetto_mobo.simulation.run`, which launches JETTO in a Singularity container. \n",
    "\n",
    "Often you want to launch many JETTO runs in parallel, so we also provide a toy version of `run_many`.\n",
    "\n",
    "The signatures and behaviour of the functions are exactly the same (although the toy ones are less reliably realistic!), so when you want to use the real function you can do a direct swap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # If you were running the full JETTO, you'd use `from jetto_mobo.simulation import run`\n",
    "from workshop_tools import run_toy\n",
    " # If you were running the full JETTO, you'd use `from jetto_mobo.simulation import run_many`\n",
    "from workshop_tools import run_many_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run` and `run_toy` take a `RunConfig` object, an output directory, and a JETTO Singularity image, and return a `JettoResults` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(run_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_many` and `run_many_toy` take a JETTO singularity image, and a `dict` of `RunConfig`s and corresponding output directories, returning a list of `JettoResults`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(run_many_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might already have a way of generating JETTO `RunConfig` objects. We found that it was a bit finicky, so we wrote a helper function to do it for me. This copies the template to a directory, and creates a `RunConfig` for the new directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop_tools import create_config_toy # Again, if you were using the full JETTO, you'd use `from jetto_mobo.simulation import create_config`\n",
    "\n",
    "test_config = create_config_toy(template=\"assets/spr45-qlknn\", directory=\"runs/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RunConfig` returned by `create_config` also has an extra property, `exfile`, that stores the path to the binary exfile. Modifying the input profiles is done by loading, editing and saving the exfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jetto_tools.binary\n",
    "import numpy as np\n",
    "\n",
    "# Load the exfile\n",
    "exfile = jetto_tools.binary.read_binary_file(test_config.exfile)\n",
    "\n",
    "# Make an EC profile that's a Gaussian bump centred on the origin\n",
    "# Note that we modify the 0th time slice of the EC profile\n",
    "exfile[\"QECE\"][0] = np.exp(-(exfile[\"XRHO\"][0]**2)/0.01)\n",
    "\n",
    "# Save the exfile\n",
    "jetto_tools.binary.write_binary_exfile(exfile, test_config.exfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `jetto_mobo` package uses `asyncio` when running the JETTO containers - this is way of managing non-blocking multiprocess execution.\n",
    "\n",
    "In a 'normal' Python script, you would need to run these functions using `asyncio.run`, as explained in the docstrings. However, in an IPython environment (such as this Jupyter notebook), you instead run the functions using the `await` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop_tools import plot_profiles # Helper function for plotting QECE/q profiles\n",
    "\n",
    "results = await run_toy(\n",
    "        run_config=test_config,\n",
    "        run_directory=\"runs/test\",\n",
    "        jetto_image=\"\", # Leave this as a blank string - in practice, you would specify the path to the JETTO Singularity image\n",
    "    )\n",
    "\n",
    "profiles = results.load_profiles()\n",
    "plot_profiles(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! We've successfully run the toy version of JETTO to simulate a $q$-profile from an ECRH profile.\n",
    "\n",
    "The `JettoResults` object returned by `run_toy` has `profiles.CDF` and `timetraces.CDF` with the following entries set:\n",
    "\n",
    "```\n",
    "profiles[\"QECE\"][-1] # Set to the ECRH profile provided as an input (i.e. profiles[\"QECE\"][0])\n",
    "profiles[\"Q\"][-1] # Set to the toy model's prediction of the q-profile\n",
    "timetraces[\"Q0\"][-1] # Set to the on-axis value of the model's prediction\n",
    "timetraces[\"QMIN\"][-1] # Set to the minimum of the model's prediction\n",
    "timetraces[\"ROQM\"][-1] # Set to the normalised radial location of the model's prediction\n",
    "```\n",
    "\n",
    "Any other entries will not have been modified from the JETTO run template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameterising input profiles\n",
    "\n",
    "`jetto-mobo` is designed for optimisation of plasma profiles. It provides a Python decorator to ensure that functions representing a profile are of the correct form. If a function does not match the decorator signature *exactly* (including type hints!), it will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.inputs import plasma_profile\n",
    "\n",
    "help(plasma_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of incorrect use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@plasma_profile\n",
    "def gaussian_profile(x, p):\n",
    "    variance = p[0]\n",
    "    mean = p[1]\n",
    "    scale = p[2]\n",
    "    return scale * np.exp(-(x - mean)**2 / variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of correct use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@plasma_profile\n",
    "def gaussian_profile(xrho: np.ndarray, parameters: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    **Remember to write an informative docstring!**\n",
    "    \n",
    "    A Gaussian profile with specified variance, mean and scale.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xrho : np.ndarray\n",
    "        Normalised radial coordinate.\n",
    "    parameters : np.ndarray\n",
    "        Array of parameters, in the order [variance, mean, scale].\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Gaussian profile.\n",
    "    \"\"\"\n",
    "    variance = parameters[0]\n",
    "    mean = parameters[1]\n",
    "    scale = parameters[2]\n",
    "    return scale * np.exp(-(xrho - mean)**2 / variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to think about \n",
    "\n",
    "When you're designing an optimisation task, the parameterisation of the input plays an important role in shaping the solutions that you generate. Some things you need to think about are:\n",
    "\n",
    "1. How versatile is it?\n",
    "\n",
    "    Does the parameterisation permit a wide range of input profiles to be represented? We want to ensure that the (unknown) optimal profile can be represented by our parameterisation, without pre-determining the kind of solution we expect.\n",
    "\n",
    "    At the same time, are most of the profiles vaguely sensible? We don't want to waste time exploring regions of space that are *a priori* unlikely to perform well.\n",
    "\n",
    "\n",
    "2. How unique is the mapping?\n",
    "\n",
    "    Does each input profile correspond to exactly one set of parameters? If not, then we end up duplicating regions of search space, which reduces efficiency.\n",
    "\n",
    "\n",
    "3. How continuous is the mapping from parameters to objectives?\n",
    "\n",
    "    We want regions that are close in parameter space to be close in objective space, because this makes it much easier for the model to capture the mapping accurately. If you tweak the parameters a little bit, the resulting objective values should also move a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: ECRH parameterisation\n",
    "\n",
    "In the cell below, define a parameterisation suitable for representing ECRH profiles. Your profile should be constrained to $[0, 1]$.\n",
    "Remember to decorate it with `@plasma_profile`!\n",
    "\n",
    "Also define a $2 \\times M$ array, where $M$ is the number of parameters, representing the lower and upper bounds of the allowed parameter ranges.\n",
    "\n",
    "Use `plot_ecrh_profile` to visualise your parameterisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jetto_mobo.inputs import plasma_profile\n",
    "from workshop_tools import plot_ecrh_profile\n",
    "bounds = np.array([[1e-4, 0, 0.1], [1e-2, 1, 1]])\n",
    "plot_ecrh_profile(gaussian_profile, bounds)\n",
    "\n",
    "# Define your ECRH profile\n",
    "# ...\n",
    "\n",
    "# Define the parameter bounds\n",
    "# parameter_bounds = ...\n",
    "\n",
    "# Visualise the profile\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Formulating objective functions\n",
    "\n",
    "An objective function takes the output of a JETTO run and returns a vector corresponding to the performance of the run against various different metrics.\n",
    "`jetto-mobo` provides another decorator to enforce compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.objectives import objective\n",
    "\n",
    "help(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator is designed for vector objectives, but also supports scalar ones.\n",
    "A vector usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from jetto_mobo.objectives import objective \n",
    "from jetto_tools.results import JettoResults\n",
    "\n",
    "def q_is_positive(profiles: nc.Dataset):\n",
    "    # profiles[\"Q\"] is a 2D array of shape (time, radius)\n",
    "    # profiles[\"Q\"][-1] is the last time slice\n",
    "    return 1 if np.all(profiles[\"Q\"][-1].data) > 0 else 0\n",
    "\n",
    "def q0_is_greater_than_2(timetraces: nc.Dataset):\n",
    "    # Some variables are stored in the timetraces Dataset\n",
    "    return 1 if timetraces[\"Q0\"][-1].data > 2 else 0\n",
    "\n",
    "@objective\n",
    "def my_fancy_objective(results: JettoResults) -> np.ndarray:\n",
    "    try:\n",
    "        # Sometimes this throws an error, when something has gone wrong with JETTO\n",
    "        # Putting it in a try/except block means that we can handle these cases\n",
    "        profiles = results.load_profiles()\n",
    "        timetraces = results.load_timetraces()\n",
    "    except:\n",
    "        # There are a few options for what to do with JETTO failures\n",
    "        # In this case, we choose to treat them as badly-scoring points,\n",
    "        # rather than discarding them entirely\n",
    "        return np.array([0, 0])\n",
    "    \n",
    "    return np.array(\n",
    "        [\n",
    "            q_is_positive(profiles),\n",
    "            q0_is_greater_than_2(timetraces),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to think about\n",
    "\n",
    "Both the choice of objective criteria and their mathematical formulation have a strong impact on the kinds of solutions that are produced. Some things to consider when designing objective functions are:\n",
    "\n",
    "1. Are any of these objectives measuring the same thing?\n",
    "\n",
    "    As the dimensionality of the objective space increases, everything becomes much more difficult. Keeping the dimensionality as low as possible ensures that the algorithm explores sufficiently.\n",
    "\n",
    "\n",
    "2. How do I want the objective value to decay?\n",
    "\n",
    "    The GP surrogate model of the input-objective mapping works best if the objective values vary smoothly.\n",
    "    It's also useful to have some objective value everywhere in space, rather than it being 0 in a lot of regions.\n",
    "\n",
    "3. Are my objectives directly comparable?\n",
    "\n",
    "    This is important for two reasons. Firstly, the acquisition function that our package uses is currently qNEHVI, which involves computing hypervolumes in objective space. If the objectives vary on dramatically different scales, the increase in hypervolume size for a modest improvement in one objective may outweigh the improvement achievable in another objective.\n",
    "    \n",
    "    Secondly, when a human comes to interpret the solutions, it is much easier to understand if all the objectives vary similarly.\n",
    "    \n",
    "    \n",
    "4. Could any of these objectives be reformulated as constraints?\n",
    "\n",
    "    In general, constraints are easier to model than objectives, thanks to the curse of dimensionality.\n",
    "    We don't cover constrained BO here, but it's quite easy to extend to (e.g. see the [BoTorch docs](https://botorch.org/tutorials/constrained_multi_objective_bo))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: $q$-profile objectives\n",
    "\n",
    "In this workshop, we're tackling the ECRH-$q$ optimisation problem, and so all our metrics will relate to the shape of the $q$ profile. Our toy model only supports computing $q$ metrics that use `QECE` and `Q` from JETTO profiles file, and `Q0`, `QMIN`, and `ROQM` from the timetraces file.\n",
    "\n",
    "Ideally, future work would also explore other impacts of the ECRH profile, but for the time being we'll stick with looking at $q$.\n",
    "\n",
    "In the cell below, define a vector objective function called `q_vector_objective` with at least 3 components that relate to properties of the $q$-profile. You might want to define a common transform that is used to rescale and normalise all the objectives, so that they vary in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jetto_mobo.objectives import objective \n",
    "from jetto_tools.results import JettoResults\n",
    "\n",
    "# Define your objective function\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: interim tests\n",
    "\n",
    "Modify the below code to test your input and objective functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop_tools import run_many_toy, create_config_toy, plot_results\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "from jetto_tools.results import JettoResults\n",
    "\n",
    "# Define a function that:\n",
    "# 1. Takes a set of parameters (of shape (batch_size, n_parameters))\n",
    "# 2. Creates a set of batch_size RunConfigs using create_config_toy\n",
    "#    (ie uses create_config_toy, and then modifies the ECRH profile in\n",
    "#     each RunConfigs' exfile using the parameters and your ECRH function)\n",
    "# 3. Runs the simulations using run_many_toy, to get a list of batch_size JettoResults objects\n",
    "# 4. Computes the objective function values using the results\n",
    "# 5. Returns a tuple containing:\n",
    "#      - The ECRH profiles as a np.ndarray of shape (batch_size, any)\n",
    "#      - The q profiles as a np.ndarray of shape (batch_size, any)\n",
    "#      - The objective function values as a np.ndarray of shape (batch_size, n_objectives)\n",
    "def evaluate(parameters: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    # Convert the parameters to a list of ECRH profiles\n",
    "    # ecrh = ...\n",
    "    # Convert the ECRH profiles to a list of RunConfigs\n",
    "    # ...\n",
    "    # Run the simulations\n",
    "    # ...\n",
    "    # Extract the q profiles\n",
    "    # q = ...\n",
    "    # Compute the objective function values\n",
    "    # objective_values = ...\n",
    "    return ecrh, q, objective_values\n",
    "\n",
    "# Define the parameter bounds\n",
    "# parameter_bounds = ...\n",
    "\n",
    "# Generate some random parameters\n",
    "batch_size = 3\n",
    "rng = np.random.default_rng(42)\n",
    "parameters = rng.uniform(parameter_bounds[:, 0], parameter_bounds[:, 1], size=(batch_size, parameter_bounds.shape(1)))\n",
    "\n",
    "# Evaluate\n",
    "ecrh, q, objective_values = evaluate(parameters)\n",
    "\n",
    "# Plot the results, adding the objective labels\n",
    "objective_labels = [...]\n",
    "plot_results(ecrh, q, objective_values, objective_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fitting a model\n",
    "\n",
    "BayesOpt uses a probabilistic model (a Gaussian process, or GP) in selecting the next candidates to trial.\n",
    "The GP provides a distribution over the predicted performance (objective values) of a candidate (in our case, ECRH profile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.surrogate import fit_surrogate_model\n",
    "\n",
    "help(fit_surrogate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a Gaussian process can be accelerated using a GPU; we use PyTorch to allow cross-compatibility between GPUs and CPUs. The downside of this is that it requires a bit more fiddling, and requires the use of the `torch` library.\n",
    "\n",
    "Quick demo of using `torch.tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = np.array([1.1, 2.2, 3.3])\n",
    "\n",
    "# Convert an array to a PyTorch tensor on the CPU\n",
    "a_cpu_tensor = torch.tensor(a, dtype=torch.float32, device=torch.device(\"cpu\"))\n",
    "\n",
    "# Convert an array to a PyTorch tensor on an NVIDIA GPU\n",
    "# a_gpu_tensor = torch.tensor(a, dtype=torch.float32, device=torch.device(\"cuda:0\"))\n",
    "\n",
    "# Convert a tensor on the CPU to a tensor on an NVIDIA GPU\n",
    "# a_gpu_tensor = a_cpu_tensor.to(device=torch.device(\"cuda:0\"))\n",
    "\n",
    "# Convert a tensor back to a numpy array\n",
    "a_again = a_cpu_tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, `X` will be the ECRH parameterisation parameters, `X_bounds` the ECRH parameter bounds, and `Y` the objective values.\n",
    "\n",
    "Let's demonstrate the fitting of a GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.surrogate import fit_surrogate_model\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go \n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'\n",
    "\n",
    "def himmelblau(x1, x2):\n",
    "    x1 = 8 * x1 - 4\n",
    "    x2 = 8 * x2 - 4\n",
    "    return np.clip(300 - (x1**2 + x2 - 11)**2 - (x1 + x2**2 - 7)**2, 0, None)\n",
    "\n",
    "# Generate some 2D data\n",
    "input_bounds = np.array([[0, 0], [1, 1]])\n",
    "x1, x2 = np.meshgrid(np.linspace(input_bounds[0, 0], input_bounds[1, 0], 100), np.linspace(input_bounds[0, 1], input_bounds[1,1], 100))\n",
    "x = np.stack([x1.flatten(), x2.flatten()], axis=-1)\n",
    "y = himmelblau(x[:, 0], x[:, 1])\n",
    "\n",
    "ground_truth_figure = go.Figure(\n",
    "    go.Contour(\n",
    "        x=x[:, 0],\n",
    "        y=x[:, 1],\n",
    "        z=y,\n",
    "        contours_coloring=\"heatmap\",\n",
    "    ),\n",
    "    layout_title=\"Example function\"\n",
    ")\n",
    "ground_truth_figure.update_layout(\n",
    "    xaxis_title=\"x1\",\n",
    "    yaxis_title=\"x2\",\n",
    ")\n",
    "ground_truth_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the data\n",
    "rng = np.random.default_rng(42)\n",
    "sample_indices = rng.choice(np.arange(x.shape[0]), size=50, replace=False)\n",
    "x_sample = x[sample_indices, :]\n",
    "y_sample = y[sample_indices]\n",
    "\n",
    "ground_truth_figure.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_sample[:, 0],\n",
    "        y=x_sample[:, 1],\n",
    "        mode=\"markers\",\n",
    "        showlegend=False,\n",
    "        marker_color=\"lightgreen\"\n",
    "    )\n",
    ")\n",
    "ground_truth_figure.update_layout(\n",
    "    xaxis_range=[0, 1],\n",
    "    yaxis_range=[0, 1],\n",
    ")\n",
    "ground_truth_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a surrogate model to the samples\n",
    "model = fit_surrogate_model(\n",
    "    X=torch.tensor(x_sample),\n",
    "    X_bounds=torch.tensor(input_bounds),\n",
    "    Y=torch.tensor(y_sample).unsqueeze(-1),\n",
    ")\n",
    "\n",
    "# Because this is a 2D function, we can visualise the surrogate quite easily\n",
    "# We plot the mean of the surrogate model's predictions, without showing the uncertainty\n",
    "model_output = model(torch.tensor(x)).mean.detach().cpu().numpy()\n",
    "\n",
    "go.Figure(\n",
    "    go.Contour(\n",
    "        x=x[:, 0],\n",
    "        y=x[:, 1],\n",
    "        z=model_output,\n",
    "        contours_coloring=\"heatmap\",\n",
    "    ),\n",
    "    layout_title=\"Surrogate's prediction\"\n",
    ").show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we had a true function (the 2D Himmelblau function, $f: \\mathbb{R}^2 \\to \\mathbb{R}$), made a few observations (green), and fit the GP model to the observations. There's no trickery here, just statistics - fitting the GP is done just by inverting and multiplying some matrices!\n",
    "\n",
    "In BayesOpt, what we have instead is a true function (the mapping from $p$ input EC parameters to $n$ objective values $\\phi: \\mathbb{R}^p \\to \\mathbb{R}^n$), which we can *selectively* observe. In the next sections we'll talk about how the observation selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating initial candidates\n",
    "\n",
    "The GP model requires some initial data to get started. Ideally this data will be widely spread throughout the input space. One method for generating initial samples is Sobol sampling, a quasirandom method that generates more evenly distributed samples than other methods (such as a pure random sampling). Other methods exist, such as Latin Hypercube sampling; for some reason, everyone seems to use Sobol in BayesOpt.\n",
    "\n",
    "| Random sampling | Sobol sampling |\n",
    "| :-------------: | :------------: |\n",
    "| ![Random sampling](assets/random.svg) | ![Sobol sampling Image](assets/sobol.svg)|\n",
    "\n",
    "`jetto-mobo` provides a wrapper for a Sobol sampling function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.acquisition import generate_initial_candidates\n",
    "\n",
    "help(generate_initial_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: initialising a GP model\n",
    "\n",
    "In this exercise, you need to generate some initial candidates using Sobol sampling, and then use the candidates to initialise a GP surrogate model of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.acquisition import generate_initial_candidates\n",
    "from workshop_tools import plot_ecrh_profile, plot_results, run_many_toy\n",
    "\n",
    "# Generate some initial candidates using your parameter_bounds vector\n",
    "# and the generate_initial_candidates function\n",
    "# Remember to convert the bounds to a torch tensor!\n",
    "# parameters = ...\n",
    "\n",
    "# Run the candidates through the simulation using your evaluate function\n",
    "ecrh, q, objective_values = evaluate(parameters)\n",
    "\n",
    "# Visualise the results using plot_results\n",
    "plot_results(ecrh, q, objective_values, objective_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these initial samples as our starting point for optimisation. To do so, we first need to fit the GP model to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.surrogate import fit_surrogate_model\n",
    "\n",
    "# Fit a surrogate model to the results\n",
    "# The surrogate model is learning the mapping from ECRH profile parameters \n",
    "# to objective values\n",
    "# ecrh_q_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimising an acquisition function\n",
    "\n",
    "The key step in BayesOpt is using the GP model to inform the selection of the next points. As a Gaussian process provides estimates of the uncertainty in its predictions of the performance of future points, an *acquisition function* can decide whether to try points that are known to be good, or points where the performance is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.acquisition import generate_trial_candidates\n",
    "\n",
    "help(generate_trial_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are a lot of options here, you really don't need to worry about most of them. All of the optional arguments have sensible defaults.\n",
    "\n",
    "Because we're looking at multi-objective optimisation, we use `jetto_mobo.acquisition.qNoisyExpectedHypervolumeImprovement` as our acquisition function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: optimising an acquisition function\n",
    "\n",
    "Using the `ecrh_q_model` that you defined previously, generate, evaluate and visualise a new set of trial candidates by optimising the qNEHVI acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.acquisition import generate_trial_candidates, qNoisyExpectedHypervolumeImprovement\n",
    "from workshop_tools import plot_results, run_many_toy\n",
    "\n",
    "# Generate some trial candidates by optimising the qNEHVI acquisition function\n",
    "# using your parameter_bounds vector and the generate_trial_candidates function\n",
    "# Remember to convert the bounds to a torch tensor!\n",
    "# new_parameters = ...\n",
    "\n",
    "# Run the candidates through the simulation using your evaluate function\n",
    "new_ecrh, new_q, new_objective_values = evaluate(new_parameters)\n",
    "\n",
    "# Visualise the results using plot_results\n",
    "plot_results(new_ecrh, new_q, new_objective_values, objective_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extracting the Pareto optimal solutions\n",
    "\n",
    "In a multi-objective problem, there are no single best solutions. Instead, solutions lie on a surface - called the Pareto front - that represents the optimal tradeoffs between objectives. These solutions are called Pareto optimal.\n",
    "\n",
    "qNEHVI and other multi-objective acquisition functions seek to find solutions that populate the Pareto front. During optimisation, the algorithm should hopefully find better and better solutions, pushing the Pareto front outwards.\n",
    "\n",
    "Once optimisation is complete, we need to filter all of the solutions that have been observed, to extract the set of solutions that are Pareto optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetto_mobo.utils import get_pareto_dominant_mask\n",
    "help(get_pareto_dominant_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the old and new results\n",
    "all_parameters = np.concatenate([parameters, new_parameters], axis=0)\n",
    "all_ecrh = np.concatenate([ecrh, new_ecrh], axis=0)\n",
    "all_q = np.concatenate([q, new_q], axis=0)\n",
    "all_objective_values = np.concatenate([objective_values, new_objective_values], axis=0)\n",
    "\n",
    "# Get the Pareto-dominant points\n",
    "is_pareto_dominant = get_pareto_dominant_mask(all_objective_values)\n",
    "\n",
    "# Visualise the results using plot_results\n",
    "plot_results(all_ecrh[is_pareto_dominant], all_q[is_pareto_dominant], all_objective_values[is_pareto_dominant], objective_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bringing it all together\n",
    "\n",
    "Congratulations! You're now ready to write a full Bayesian optimisation routine.\n",
    "\n",
    "Remember, the method is:\n",
    "\n",
    "1. Define a parameterisation for your input\n",
    "2. Define your objective values\n",
    "3. Generate some initial Sobol samples of the input\n",
    "4. Observe the objective values of the initial samples\n",
    "5. Fit a GP to the initial data\n",
    "6. Optimise the acquisition function to produce some new candidate parameters\n",
    "7. Observe the objective values of the new candidates\n",
    "8. Fit a GP to all the data seen to date\n",
    "9. Repeat from (6) until a good enough solution (or your compute budget) is reached!\n",
    "10. Filter out the Pareto optimal results\n",
    "\n",
    "In the cell below, write a loop to run steps 6-9 for 10 steps of 10 candidates, storing the results as you go. Then, filter out the Pareto optimal results and display them with `plot_results`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetto-mobo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
